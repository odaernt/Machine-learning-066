{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_read = pd.read_csv('new_features_datasets/F_test.csv')\n",
    "training_and_validation_read = pd.read_csv('new_features_datasets/F_training_and_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = training_and_validation_read.columns.intersection(test_read.columns)\n",
    "\n",
    "# Reorder the columns in both DataFrames to match the order of 'common_columns'\n",
    "test = test_read[common_columns]\n",
    "training_and_validation = training_and_validation_read[common_columns]\n",
    "\n",
    "# Add the 'pv_measurement' column from 'test' to 'training_and_validation'\n",
    "training_and_validation['pv_measurement'] = training_and_validation_read['pv_measurement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_and_validation['date_forecast'] = pd.to_datetime(training_and_validation['date_forecast'])\n",
    "\n",
    "# Define the date for the split\n",
    "split_date = pd.to_datetime('2022-10-22')\n",
    "\n",
    "# Split the DataFrame into training and test sets\n",
    "train_fit = training_and_validation[training_and_validation['date_forecast'] <= split_date]\n",
    "train_fit.reset_index(drop=True, inplace=True)\n",
    "X_train = train_fit.drop(columns=['pv_measurement'])\n",
    "X_train = X_train.drop(columns=['date_forecast'])\n",
    "\n",
    "y_train = train_fit['pv_measurement']  # Target variable\n",
    "test_fit = training_and_validation[training_and_validation['date_forecast'] > split_date]\n",
    "test_fit.reset_index(drop=True, inplace=True)\n",
    "X_test = test_fit.drop(columns=['pv_measurement'])\n",
    "X_test = X_test.drop(columns=['date_forecast'])\n",
    "\n",
    "y_test = test_fit['pv_measurement']  # Target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error on validation data: 83.32\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Step 2: Create and train the Random Forest model\n",
    "dt_model = DecisionTreeRegressor()\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Evaluate the model's performance on the validation data\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error (MSE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Mean absolute error on validation data: {mae:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBooster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error on validation data: 79.68\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# Step 2: Create and train the Gradient Boosting model\n",
    "gb_model = GradientBoostingRegressor()\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Evaluate the model's performance on the validation data\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Calculate the mean absolute error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Mean absolute error on validation data: {mae:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Feature  Importance\n",
      "8                      direct_rad:W    0.369722\n",
      "42                       Location_A    0.283722\n",
      "11                      elevation:m    0.166571\n",
      "3                   clear_sky_rad:W    0.104497\n",
      "6                     diffuse_rad:W    0.050281\n",
      "32                    sun_azimuth:d    0.006933\n",
      "37                     visibility:m    0.005336\n",
      "33                  sun_elevation:d    0.002193\n",
      "35                      t_1000hPa:K    0.001213\n",
      "45                             Year    0.001080\n",
      "2             clear_sky_energy_1h:J    0.001030\n",
      "7                  diffuse_rad_1h:J    0.000990\n",
      "21             precip_type_5min:idx    0.000900\n",
      "9                   direct_rad_1h:J    0.000843\n",
      "0          absolute_humidity_2m:gm3    0.000738\n",
      "5                    dew_point_2m:K    0.000662\n",
      "23                 pressure_50m:hPa    0.000547\n",
      "39              wind_speed_u_10m:ms    0.000318\n",
      "38                wind_speed_10m:ms    0.000315\n",
      "22                pressure_100m:hPa    0.000288\n",
      "51                          Month_6    0.000258\n",
      "36              total_cloud_cover:p    0.000211\n",
      "54                          Month_9    0.000206\n",
      "63                            Day_6    0.000204\n",
      "19                 msl_pressure:hPa    0.000198\n",
      "55                         Month_10    0.000119\n",
      "27                 sfc_pressure:hPa    0.000106\n",
      "83                           Day_26    0.000095\n",
      "52                          Month_7    0.000070\n",
      "10          effective_cloud_cover:p    0.000058\n",
      "70                           Day_13    0.000055\n",
      "96                           Hour_8    0.000044\n",
      "97                           Hour_9    0.000041\n",
      "26      relative_humidity_1000hPa:p    0.000036\n",
      "53                          Month_8    0.000033\n",
      "58                            Day_1    0.000033\n",
      "43                       Location_B    0.000031\n",
      "98                          Hour_10    0.000017\n",
      "105                         Hour_17    0.000006\n",
      "94                           Hour_6    0.000003\n",
      "100                         Hour_12    0.000001\n",
      "86                           Day_29    0.000000\n",
      "87                           Day_30    0.000000\n",
      "85                           Day_28    0.000000\n",
      "103                         Hour_15    0.000000\n",
      "84                           Day_27    0.000000\n",
      "82                           Day_25    0.000000\n",
      "80                           Day_23    0.000000\n",
      "79                           Day_22    0.000000\n",
      "78                           Day_21    0.000000\n",
      "77                           Day_20    0.000000\n",
      "76                           Day_19    0.000000\n",
      "75                           Day_18    0.000000\n",
      "81                           Day_24    0.000000\n",
      "91                           Hour_3    0.000000\n",
      "111                         Hour_23    0.000000\n",
      "88                           Day_31    0.000000\n",
      "102                         Hour_14    0.000000\n",
      "101                         Hour_13    0.000000\n",
      "99                          Hour_11    0.000000\n",
      "104                         Hour_16    0.000000\n",
      "106                         Hour_18    0.000000\n",
      "73                           Day_16    0.000000\n",
      "107                         Hour_19    0.000000\n",
      "108                         Hour_20    0.000000\n",
      "109                         Hour_21    0.000000\n",
      "95                           Hour_7    0.000000\n",
      "110                         Hour_22    0.000000\n",
      "93                           Hour_5    0.000000\n",
      "92                           Hour_4    0.000000\n",
      "90                           Hour_2    0.000000\n",
      "89                           Hour_1    0.000000\n",
      "74                           Day_17    0.000000\n",
      "56                         Month_11    0.000000\n",
      "72                           Day_15    0.000000\n",
      "20                   precip_5min:mm    0.000000\n",
      "34   super_cooled_liquid_water:kgm2    0.000000\n",
      "31                  snow_water:kgm2    0.000000\n",
      "30               snow_melt_10min:mm    0.000000\n",
      "29                   snow_drift:idx    0.000000\n",
      "28                    snow_depth:cm    0.000000\n",
      "25                  rain_water:kgm2    0.000000\n",
      "24                      prob_rime:p    0.000000\n",
      "18                 is_in_shadow:idx    0.000000\n",
      "41          wind_speed_w_1000hPa:ms    0.000000\n",
      "17                       is_day:idx    0.000000\n",
      "16                 fresh_snow_6h:cm    0.000000\n",
      "15                 fresh_snow_3h:cm    0.000000\n",
      "14                fresh_snow_24h:cm    0.000000\n",
      "13                 fresh_snow_1h:cm    0.000000\n",
      "12                fresh_snow_12h:cm    0.000000\n",
      "4                   dew_or_rime:idx    0.000000\n",
      "40              wind_speed_v_10m:ms    0.000000\n",
      "44                       Location_C    0.000000\n",
      "71                           Day_14    0.000000\n",
      "61                            Day_4    0.000000\n",
      "69                           Day_12    0.000000\n",
      "68                           Day_11    0.000000\n",
      "67                           Day_10    0.000000\n",
      "66                            Day_9    0.000000\n",
      "65                            Day_8    0.000000\n",
      "64                            Day_7    0.000000\n",
      "62                            Day_5    0.000000\n",
      "60                            Day_3    0.000000\n",
      "46                          Month_1    0.000000\n",
      "59                            Day_2    0.000000\n",
      "57                         Month_12    0.000000\n",
      "1               air_density_2m:kgm3    0.000000\n",
      "50                          Month_5    0.000000\n",
      "49                          Month_4    0.000000\n",
      "48                          Month_3    0.000000\n",
      "47                          Month_2    0.000000\n",
      "112                         Hour_24    0.000000\n"
     ]
    }
   ],
   "source": [
    "feature_importance = gb_model.feature_importances_\n",
    "your_feature_names = X_test.columns.tolist()\n",
    "feature_names = pd.DataFrame({'Feature': your_feature_names, 'Importance': feature_importance})\n",
    "# Sort by importance in descending order\n",
    "feature_names = feature_names.sort_values(by='Importance', ascending=False)\n",
    "# Print or view the sorted feature importance\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training2 = training_and_validation.drop(columns=['pv_measurement'])\n",
    "x_training = training2.drop(columns=['date_forecast'])\n",
    "x_training.reset_index(drop=True, inplace=True)\n",
    "\n",
    "y_training = training_and_validation['pv_measurement']  # Target variable\n",
    "y_training.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#test = test.drop(columns = ['date_forecast'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error on validation data: 147.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# Step 2: Create and train the Gradient Boosting model\n",
    "knn_model=KNeighborsClassifier()\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Evaluate the model's performance on the validation data\n",
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "# Calculate the mean absolute error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Mean absolute error on validation data: {mae:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Step 2: Create and train the Gradient Boosting model\\nsv_model=SVC(C=5, gamma =1, kernel= 'rbf')\\nsv_model.fit(X_train, y_train)\\n\\n# Step 3: Evaluate the model's performance on the validation data\\ny_pred = sv_model.predict(X_test)\\n\\n# Calculate the mean absolute error (MAE)\\nmae = mean_absolute_error(y_test, y_pred)\\nprint(f'Mean absolute error on validation data: {mae:.2f}')\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Step 2: Create and train the Gradient Boosting model\n",
    "sv_model=SVC(C=5, gamma =1, kernel= 'rbf')\n",
    "sv_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Evaluate the model's performance on the validation data\n",
    "y_pred = sv_model.predict(X_test)\n",
    "\n",
    "# Calculate the mean absolute error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Mean absolute error on validation data: {mae:.2f}')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error on validation data: 273.89\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create and train the Gradient Boosting model\n",
    "nb_model=GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Evaluate the model's performance on the validation data\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Calculate the mean absolute error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Mean absolute error on validation data: {mae:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: [1085, 1222, 1369, 1394, 1532, 1548, 1653, 1708, 1812, 1814, 1933, 1950, 1978, 1992, 1993, 2034, 2050, 2052, 2167, 2199, 2318, 2435, 2445, 2493, 2567, 2586, 2795, 2810, 2825, 2864, 2866, 2914, 2939, 2968, 2970, 2972, 3057, 3181, 3254, 3334, 3374, 3385, 3416, 3419, 3526, 3608, 3619, 3647, 3666, 3747, 3765, 3846, 3866, 3948, 3949, 3953, 3968, 3984, 4008, 4209, 4221, 4250, 4318, 4335, 4413, 4420, 4475, 4510, 4533, 4565, 4569, 4574, 4608, 4612, 4635, 4690, 4706, 4709, 4807, 4842, 4848, 4861, 4881, 4890, 4895, 4897, 4976, 5002, 5043]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29544\\1777538920.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Transform y_test with the same encoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0my_test_encoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Now, proceed with training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\malin\\OneDrive - NTNU\\5.klasse\\Høst 2023\\Maskinlæring\\Main exercise\\Solar prediction\\.venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\malin\\OneDrive - NTNU\\5.klasse\\Høst 2023\\Maskinlæring\\Main exercise\\Solar prediction\\.venv\\lib\\site-packages\\sklearn\\utils\\_encode.py\u001b[0m in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_unknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"y contains previously unseen labels: {str(diff)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: y contains previously unseen labels: [1085, 1222, 1369, 1394, 1532, 1548, 1653, 1708, 1812, 1814, 1933, 1950, 1978, 1992, 1993, 2034, 2050, 2052, 2167, 2199, 2318, 2435, 2445, 2493, 2567, 2586, 2795, 2810, 2825, 2864, 2866, 2914, 2939, 2968, 2970, 2972, 3057, 3181, 3254, 3334, 3374, 3385, 3416, 3419, 3526, 3608, 3619, 3647, 3666, 3747, 3765, 3846, 3866, 3948, 3949, 3953, 3968, 3984, 4008, 4209, 4221, 4250, 4318, 4335, 4413, 4420, 4475, 4510, 4533, 4565, 4569, 4574, 4608, 4612, 4635, 4690, 4706, 4709, 4807, 4842, 4848, 4861, 4881, 4890, 4895, 4897, 4976, 5002, 5043]"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize a label encoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder to y_train and transform it\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "\n",
    "# Transform y_test with the same encoder\n",
    "y_test_encoded = encoder.transform(y_test)\n",
    "\n",
    "# Now, proceed with training\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_model.fit(X_train, y_train_encoded)\n",
    "y_pred_encoded = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate MAE with the encoded labels\n",
    "mae = mean_absolute_error(y_test_encoded, y_pred_encoded)\n",
    "print(f'Mean absolute error on validation data: {mae:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error on validation data: 169.24\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Ensure your data is sorted chronologically if it's a time series.\n",
    "\n",
    "# Step 2: Create and train the ARIMA model\n",
    "# Here, p, d, and q are the parameters for ARIMA:\n",
    "# p is the number of lag observations included (lag order)\n",
    "# d is the number of times that the raw observations are differenced (degree of differencing)\n",
    "# q is the size of the moving average window (order of moving average)\n",
    "# You may need to experiment and tune these parameters.\n",
    "p, d, q = 1, 1, 1\n",
    "\n",
    "arima_model = ARIMA(y_train, order=(p,d,q))\n",
    "arima_model_fit = arima_model.fit()\n",
    "\n",
    "# Step 3: Evaluate the model's performance on the validation data\n",
    "y_pred = arima_model_fit.forecast(steps=len(y_test))\n",
    "\n",
    "# Calculate the mean absolute error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Mean absolute error on validation data: {mae:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 11.9 GiB for an array with shape (90785, 17521) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5852\\2252089876.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0msarima_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSARIMAX\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseasonal_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitialization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'approximate_diffuse'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0msarima_model_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msarima_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Step 3: Evaluate the model's performance on the validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\malin\\OneDrive - NTNU\\5.klasse\\Høst 2023\\Maskinlæring\\Main exercise\\Solar prediction\\.venv\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, start_params, transformed, includes_fixed, cov_type, cov_kwds, method, maxiter, full_output, disp, callback, return_params, optim_score, optim_complex_step, optim_hessian, flags, low_memory, **kwargs)\u001b[0m\n\u001b[0;32m    648\u001b[0m         \"\"\"\n\u001b[0;32m    649\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstart_params\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m             \u001b[0mstart_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m             \u001b[0mtransformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m             \u001b[0mincludes_fixed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\malin\\OneDrive - NTNU\\5.klasse\\Høst 2023\\Maskinlæring\\Main exercise\\Solar prediction\\.venv\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py\u001b[0m in \u001b[0;36mstart_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    985\u001b[0m                 \u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk_seasonal_ar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolynomial_seasonal_ar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    986\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk_seasonal_ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolynomial_seasonal_ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 987\u001b[1;33m                 warning_description='seasonal ARMA'))\n\u001b[0m\u001b[0;32m    988\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m         \u001b[1;31m# If we have estimated non-stationary start parameters but enforce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\malin\\OneDrive - NTNU\\5.klasse\\Høst 2023\\Maskinlæring\\Main exercise\\Solar prediction\\.venv\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py\u001b[0m in \u001b[0;36m_conditional_sum_squares\u001b[1;34m(endog, k_ar, polynomial_ar, k_ma, polynomial_ma, k_trend, trend_data, warning_description)\u001b[0m\n\u001b[0;32m    835\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mk_ma\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m                     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mendog\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 837\u001b[1;33m                     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlagmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'both'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    838\u001b[0m                     \u001b[0mparams_ar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpinv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m                     \u001b[0mresiduals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_ar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\malin\\OneDrive - NTNU\\5.klasse\\Høst 2023\\Maskinlæring\\Main exercise\\Solar prediction\\.venv\\lib\\site-packages\\statsmodels\\tsa\\tsatools.py\u001b[0m in \u001b[0;36mlagmat\u001b[1;34m(x, maxlag, trim, original, use_pandas)\u001b[0m\n\u001b[0;32m    395\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmaxlag\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mnobs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"maxlag should be < nobs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m     \u001b[0mlm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnobs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmaxlag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnvar\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmaxlag\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxlag\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m         lm[\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 11.9 GiB for an array with shape (90785, 17521) and data type float64"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Ensure your data is sorted chronologically.\n",
    "\n",
    "# Step 2: Create and train the SARIMA model\n",
    "p, d, q = 1, 1, 1\n",
    "P, D, Q, s = 1, 1, 1, 8760  # s is set to 8760 for hourly data with yearly seasonality\n",
    "\n",
    "sarima_model = SARIMAX(y_train, order=(p,d,q), seasonal_order=(P,D,Q,s), initialization='approximate_diffuse')\n",
    "sarima_model_fit = sarima_model.fit(low_memory=True)\n",
    "\n",
    "# Step 3: Evaluate the model's performance on the validation data\n",
    "y_pred = sarima_model_fit.forecast(steps=len(y_test))\n",
    "\n",
    "# Calculate the mean absolute error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Mean absolute error on validation data: {mae:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Scaling data can often help SVM converge faster and perform better\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 2: Create and train the Support Vector Machine model with a linear kernel and other modified parameters\n",
    "svr_model = SVR(kernel='linear', C=1.0, tol=0.01)  # Adjust parameters as needed\n",
    "svr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 3: Evaluate the model's performance on the validation data\n",
    "y_pred = svr_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the mean absolute error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Mean absolute error on validation data: {mae:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 100.0\n",
      "Mean absolute error on validation data: 6905.65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Define alphas that you want to test\n",
    "alphas = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "# Step 2: Create and train the Ridge Regression model with cross-validation\n",
    "ridge_cv_model = RidgeCV(alphas=alphas, store_cv_values=True)\n",
    "ridge_cv_model.fit(X_train, y_train)\n",
    "\n",
    "# Print the best alpha value\n",
    "print(f'Best alpha: {ridge_cv_model.alpha_}')\n",
    "\n",
    "# Step 3: Evaluate the model's performance on the validation data\n",
    "y_pred = ridge_cv_model.predict(X_test)\n",
    "\n",
    "# Calculate the mean absolute error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Mean absolute error on validation data: {mae:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fbprophet"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for fbprophet (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [60 lines of output]\n",
      "      C:\\Users\\malin\\AppData\\Local\\Temp\\pip-install-90rfyfv_\\fbprophet_dc0f7dc46c664c1c967dcadb8fa3d06f\\setup.py:10: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "        from pkg_resources import (\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib\n",
      "      creating build\\lib\\fbprophet\n",
      "      creating build\\lib\\fbprophet\\stan_model\n",
      "      Traceback (most recent call last):\n",
      "        File \"c:\\users\\malin\\onedrive - ntnu\\5.klasse\\hÃ¸st 2023\\maskinlÃ¦ring\\main exercise\\solar prediction\\.venv\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "        File \"c:\\users\\malin\\onedrive - ntnu\\5.klasse\\hÃ¸st 2023\\maskinlÃ¦ring\\main exercise\\solar prediction\\.venv\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "        File \"c:\\users\\malin\\onedrive - ntnu\\5.klasse\\hÃ¸st 2023\\maskinlÃ¦ring\\main exercise\\solar prediction\\.venv\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 252, in build_wheel\n",
      "          metadata_directory)\n",
      "        File \"C:\\Users\\malin\\AppData\\Local\\Temp\\pip-build-env-rd59nrz9\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 417, in build_wheel\n",
      "          wheel_directory, config_settings)\n",
      "        File \"C:\\Users\\malin\\AppData\\Local\\Temp\\pip-build-env-rd59nrz9\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 401, in _build_with_temp_dir\n",
      "          self.run_setup()\n",
      "        File \"C:\\Users\\malin\\AppData\\Local\\Temp\\pip-build-env-rd59nrz9\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 488, in run_setup\n",
      "          self).run_setup(setup_script=setup_script)\n",
      "        File \"C:\\Users\\malin\\AppData\\Local\\Temp\\pip-build-env-rd59nrz9\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 338, in run_setup\n",
      "          exec(code, locals())\n",
      "        File \"<string>\", line 149, in <module>\n",
      "        File \"C:\\Users\\malin\\AppData\\Local\\Temp\\pip-build-env-rd59nrz9\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\", line 107, in setup\n",
      "          return distutils.core.setup(**attrs)\n",
      "        File \"C:\\Users\\malin\\AppData\\Local\\Temp\\pip-build-env-rd59nrz9\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 185, in setup\n",
      "          return run_commands(dist)\n",
      "        File \"C:\\Users\\malin\\AppData\\Local\\Temp\\pip-build-env-rd59nrz9\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 201, in run_commands\n",
      "          dist.run_commands()\n",
      "        File \"C:\\Users\\malin\\AppData\\Local\\Temp\\pip-build-env-rd59nrz9\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 969, in run_commands\n",
      "          self.run_command(cmd)\n",
      "        File \"C:\\Users\\malin\\AppData\\Local\\Temp\\pip-build-env-rd59nrz9\\overlay\\Lib\\site-packages\\setuptools\\dist.py\", line 1234, in run_command\n",
      "          super().run_command(command)\n",
      "        File \"C:\\Users\\malin\\AppData\\Local\\Temp\\pip-build-env-rd59nrz9\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 988, in run_command\n",
      "          cmd_obj.run()\n",
      "        File \"C:\\Users\\malin\\AppData\\Local\\Temp\\pip-build-env-rd59nrz9\\overlay\\Lib\\site-packages\\wheel\\bdist_wheel.py\", line 364, in run\n",
      "          self.run_command(\"build\")\n",
      "        File \"C:\\Users\\malin\\AppData\\Local\\Temp\\pip-build-env-rd59nrz9\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 318, in run_command\n",
      "          self.distribution.run_command(command)\n",
      "        File \"C:\\Users\\malin\\AppData\\Local\\Temp\\pip-build-env-rd59nrz9\\overlay\\Lib\\site-packages\\setuptools\\dist.py\", line 1234, in run_command\n",
      "          super().run_command(command)\n",
      "        File \"C:\\Users\\malin\\AppData\\Local\\Temp\\pip-build-env-rd59nrz9\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 988, in run_command\n",
      "          cmd_obj.run()\n",
      "        File \"C:\\Users\\malin\\AppData\\Local\\Temp\\pip-build-env-rd59nrz9\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\command\\build.py\", line 131, in run\n",
      "          self.run_command(cmd_name)\n",
      "        File \"C:\\Users\\malin\\AppData\\Local\\Temp\\pip-build-env-rd59nrz9\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 318, in run_command\n",
      "          self.distribution.run_command(command)\n",
      "        File \"C:\\Users\\malin\\AppData\\Local\\Temp\\pip-build-env-rd59nrz9\\overlay\\Lib\\site-packages\\setuptools\\dist.py\", line 1234, in run_command\n",
      "          super().run_command(command)\n",
      "        File \"C:\\Users\\malin\\AppData\\Local\\Temp\\pip-build-env-rd59nrz9\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 988, in run_command\n",
      "          cmd_obj.run()\n",
      "        File \"<string>\", line 48, in run\n",
      "        File \"<string>\", line 36, in build_models\n",
      "        File \"C:\\Users\\malin\\AppData\\Local\\Temp\\pip-install-90rfyfv_\\fbprophet_dc0f7dc46c664c1c967dcadb8fa3d06f\\fbprophet\\__init__.py\", line 8, in <module>\n",
      "          from fbprophet.forecaster import Prophet\n",
      "        File \"C:\\Users\\malin\\AppData\\Local\\Temp\\pip-install-90rfyfv_\\fbprophet_dc0f7dc46c664c1c967dcadb8fa3d06f\\fbprophet\\forecaster.py\", line 14, in <module>\n",
      "          import numpy as np\n",
      "      ModuleNotFoundError: No module named 'numpy'\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for fbprophet\n",
      "ERROR: Could not build wheels for fbprophet, which is required to install pyproject.toml-based projects\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached fbprophet-0.7.1.tar.gz (64 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting Cython>=0.22 (from fbprophet)\n",
      "  Obtaining dependency information for Cython>=0.22 from https://files.pythonhosted.org/packages/bd/48/a32de24cffce9da801a7fe9c509d81c2d9fdc63954155076b061091c0d5b/Cython-3.0.4-cp37-cp37m-win_amd64.whl.metadata\n",
      "  Using cached Cython-3.0.4-cp37-cp37m-win_amd64.whl.metadata (3.2 kB)\n",
      "Collecting cmdstanpy==0.9.5 (from fbprophet)\n",
      "  Using cached cmdstanpy-0.9.5-py3-none-any.whl (37 kB)\n",
      "Collecting pystan>=2.14 (from fbprophet)\n",
      "  Using cached pystan-3.3.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\malin\\onedrive - ntnu\\5.klasse\\høst 2023\\maskinlæring\\main exercise\\solar prediction\\.venv\\lib\\site-packages (from fbprophet) (1.21.6)\n",
      "Requirement already satisfied: pandas>=1.0.4 in c:\\users\\malin\\onedrive - ntnu\\5.klasse\\høst 2023\\maskinlæring\\main exercise\\solar prediction\\.venv\\lib\\site-packages (from fbprophet) (1.3.5)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in c:\\users\\malin\\onedrive - ntnu\\5.klasse\\høst 2023\\maskinlæring\\main exercise\\solar prediction\\.venv\\lib\\site-packages (from fbprophet) (3.5.3)\n",
      "Collecting LunarCalendar>=0.0.9 (from fbprophet)\n",
      "  Using cached LunarCalendar-0.0.9-py2.py3-none-any.whl (18 kB)\n",
      "Collecting convertdate>=2.1.2 (from fbprophet)\n",
      "  Using cached convertdate-2.4.0-py3-none-any.whl (47 kB)\n",
      "Collecting holidays>=0.10.2 (from fbprophet)\n",
      "  Obtaining dependency information for holidays>=0.10.2 from https://files.pythonhosted.org/packages/7b/27/561391c502dc8bf5d931e42284188ef00c078248407b9741d611a6d9674f/holidays-0.27.1-py3-none-any.whl.metadata\n",
      "  Using cached holidays-0.27.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting setuptools-git>=1.2 (from fbprophet)\n",
      "  Using cached setuptools_git-1.2-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in c:\\users\\malin\\onedrive - ntnu\\5.klasse\\høst 2023\\maskinlæring\\main exercise\\solar prediction\\.venv\\lib\\site-packages (from fbprophet) (2.8.2)\n",
      "Collecting tqdm>=4.36.1 (from fbprophet)\n",
      "  Obtaining dependency information for tqdm>=4.36.1 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting pymeeus<=1,>=0.3.13 (from convertdate>=2.1.2->fbprophet)\n",
      "  Using cached PyMeeus-0.5.12-py3-none-any.whl\n",
      "Collecting ephem>=3.7.5.3 (from LunarCalendar>=0.0.9->fbprophet)\n",
      "  Obtaining dependency information for ephem>=3.7.5.3 from https://files.pythonhosted.org/packages/46/e1/7ef955d96e7dea153d4d2c0486bf0d7c2946e904485269ff588f178e9f3f/ephem-4.1.5-cp37-cp37m-win_amd64.whl.metadata\n",
      "  Using cached ephem-4.1.5-cp37-cp37m-win_amd64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pytz in c:\\users\\malin\\onedrive - ntnu\\5.klasse\\høst 2023\\maskinlæring\\main exercise\\solar prediction\\.venv\\lib\\site-packages (from LunarCalendar>=0.0.9->fbprophet) (2023.3.post1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\malin\\onedrive - ntnu\\5.klasse\\høst 2023\\maskinlæring\\main exercise\\solar prediction\\.venv\\lib\\site-packages (from matplotlib>=2.0.0->fbprophet) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\malin\\onedrive - ntnu\\5.klasse\\høst 2023\\maskinlæring\\main exercise\\solar prediction\\.venv\\lib\\site-packages (from matplotlib>=2.0.0->fbprophet) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\malin\\onedrive - ntnu\\5.klasse\\høst 2023\\maskinlæring\\main exercise\\solar prediction\\.venv\\lib\\site-packages (from matplotlib>=2.0.0->fbprophet) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\malin\\onedrive - ntnu\\5.klasse\\høst 2023\\maskinlæring\\main exercise\\solar prediction\\.venv\\lib\\site-packages (from matplotlib>=2.0.0->fbprophet) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\malin\\onedrive - ntnu\\5.klasse\\høst 2023\\maskinlæring\\main exercise\\solar prediction\\.venv\\lib\\site-packages (from matplotlib>=2.0.0->fbprophet) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\malin\\onedrive - ntnu\\5.klasse\\høst 2023\\maskinlæring\\main exercise\\solar prediction\\.venv\\lib\\site-packages (from matplotlib>=2.0.0->fbprophet) (3.1.1)\n",
      "Collecting aiohttp<4.0,>=3.6 (from pystan>=2.14->fbprophet)\n",
      "  Obtaining dependency information for aiohttp<4.0,>=3.6 from https://files.pythonhosted.org/packages/51/4c/e7f74722f82269f2482ef321daff341d5461a40af68bb7f9d016b98fba9c/aiohttp-3.8.6-cp37-cp37m-win_amd64.whl.metadata\n",
      "  Using cached aiohttp-3.8.6-cp37-cp37m-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting clikit<0.7,>=0.6 (from pystan>=2.14->fbprophet)\n",
      "  Using cached clikit-0.6.2-py2.py3-none-any.whl (91 kB)\n",
      "INFO: pip is looking at multiple versions of pystan to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pystan>=2.14 (from fbprophet)\n",
      "  Using cached pystan-3.2.0-py3-none-any.whl (13 kB)\n",
      "  Using cached pystan-3.1.1-py3-none-any.whl (13 kB)\n",
      "  Using cached pystan-3.1.0-py3-none-any.whl (13 kB)\n",
      "  Using cached pystan-3.0.2-py3-none-any.whl (13 kB)\n",
      "  Using cached pystan-3.0.1-py3-none-any.whl (12 kB)\n",
      "  Using cached pystan-3.0.0-py3-none-any.whl (12 kB)\n",
      "  Using cached pystan-2.19.1.1-cp37-cp37m-win_amd64.whl (79.8 MB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\malin\\onedrive - ntnu\\5.klasse\\høst 2023\\maskinlæring\\main exercise\\solar prediction\\.venv\\lib\\site-packages (from python-dateutil>=2.8.0->fbprophet) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\malin\\onedrive - ntnu\\5.klasse\\høst 2023\\maskinlæring\\main exercise\\solar prediction\\.venv\\lib\\site-packages (from tqdm>=4.36.1->fbprophet) (0.4.6)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\malin\\onedrive - ntnu\\5.klasse\\høst 2023\\maskinlæring\\main exercise\\solar prediction\\.venv\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->fbprophet) (4.7.1)\n",
      "Using cached Cython-3.0.4-cp37-cp37m-win_amd64.whl (2.7 MB)\n",
      "Using cached holidays-0.27.1-py3-none-any.whl (598 kB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Using cached ephem-4.1.5-cp37-cp37m-win_amd64.whl (1.4 MB)\n",
      "Building wheels for collected packages: fbprophet\n",
      "  Building wheel for fbprophet (pyproject.toml): started\n",
      "  Building wheel for fbprophet (pyproject.toml): finished with status 'error'\n",
      "Failed to build fbprophet\n"
     ]
    }
   ],
   "source": [
    "# Install and import necessary libraries\n",
    "!pip install fbprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fbprophet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5852\\3193461961.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfbprophet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProphet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Assuming X_train, y_train, X_test, and y_test are already loaded as pandas DataFrames or Series\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fbprophet'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from fbprophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Assuming X_train, y_train, X_test, and y_test are already loaded as pandas DataFrames or Series\n",
    "\n",
    "# Format data for Prophet: 'ds' for timestamp and 'y' for the value\n",
    "train_df = pd.DataFrame({\n",
    "    'ds': X_train['your_timestamp_column'].values,\n",
    "    'y': y_train.values\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'ds': X_test['your_timestamp_column'].values,\n",
    "    'y': y_test.values\n",
    "})\n",
    "\n",
    "# Create and train the Prophet model with yearly seasonality\n",
    "prophet = Prophet(yearly_seasonality=True)\n",
    "prophet.fit(train_df)\n",
    "\n",
    "# Generate future dataframe for predictions\n",
    "future = prophet.make_future_dataframe(periods=len(test_df))\n",
    "\n",
    "# Predict\n",
    "forecast = prophet.predict(future)\n",
    "\n",
    "# Extract predicted values for the test set\n",
    "y_pred = forecast['yhat'][-len(test_df):].values\n",
    "\n",
    "# Evaluate the model's performance on the validation data\n",
    "mae = mean_absolute_error(test_df['y'], y_pred)\n",
    "print(f'Mean absolute error on validation data: {mae:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
